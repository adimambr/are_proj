{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c433b647-4686-4dee-a91d-7361a160f72c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "BRONZE FARMS"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "volume_path = f\"/Volumes/{catalog_name}/raw/raw_data/farms.csv\"\n",
    "\n",
    "farms_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", False)\n",
    "         .csv(volume_path)\n",
    ")\n",
    "\n",
    "farms_df.write.mode(\"overwrite\").saveAsTable(f'{catalog_name}.bronze.farms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d88fd3-45d9-46ff-8e5b-9bd02a53fac1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SILVER FARMS"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from datetime import date\n",
    "\n",
    "# Read Bronze table \n",
    "farms_bronze_df = (\n",
    "    spark.read\n",
    "         .format(\"delta\")\n",
    "         .table(f'{catalog_name}.bronze.farms')\n",
    ")\n",
    "\n",
    "#Apply schema\n",
    "farms_bronze_df = (\n",
    "    farms_bronze_df\n",
    "        .select(\n",
    "            col(\"farm_id\").cast(\"string\"),\n",
    "            col(\"region\").cast(\"string\"),\n",
    "            col(\"crop_type\").cast(\"string\"),\n",
    "            col(\"area_hectares\").cast(\"long\"),\n",
    "            col(\"owner_name\").cast(\"string\"),\n",
    "            col(\"start_date\").cast(\"date\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Validation\n",
    "today = date.today()\n",
    "\n",
    "valid_crop_types = [\"Corn\", \"Soy\", \"Wheat\", \"Barley\", \"Peas\", \"Rye\", \"Oats\", \"Potatoes\", \"Carrots\"]\n",
    "valid_date_condition = (\n",
    "    (col(\"start_date\").isNotNull()) &\n",
    "    (col(\"start_date\") >= \"2000-01-01\") &\n",
    "    (col(\"start_date\") <= str(today))\n",
    ")\n",
    "\n",
    "validated_df = farms_bronze_df.withColumn(\n",
    "    \"validation_status\",\n",
    "    when(\n",
    "        (col(\"farm_id\").isNotNull()) &\n",
    "        (col(\"region\").isNotNull()) &\n",
    "        (col(\"crop_type\").isin(valid_crop_types)) &\n",
    "        (col(\"area_hectares\") > 0) &\n",
    "        valid_date_condition,\n",
    "        lit(\"valid\")\n",
    "    ).otherwise(lit(\"invalid\"))\n",
    ")\n",
    "\n",
    "# Handle duplicates \n",
    "validated_df = validated_df.dropDuplicates()\n",
    "\n",
    "# Write to Silver layer \n",
    "validated_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f'{catalog_name}.silver.farms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fe39f5-fa07-4e63-9733-45d54bd1ffa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "BRONZE SENSORS"
    }
   },
   "outputs": [],
   "source": [
    "volume_path = f\"/Volumes/{catalog_name}/raw/raw_data/sensor_readings.csv\"\n",
    "\n",
    "sensor_readings_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", False)\n",
    "         .csv(volume_path)\n",
    ")\n",
    "sensor_readings_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.bronze.sensor_readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835da594-524d-4323-b97a-16df70f3b9df",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762034859607}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1762035647536}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "SILVER SENSORS"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, lit, trim\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# Function for timestamp standardization\n",
    "def standardise_timestamp(df, col_name, formats):\n",
    "    parsed_ts = F.coalesce(*[\n",
    "        F.expr(f\"try_to_timestamp({col_name}, '{fmt}')\") for fmt in formats\n",
    "    ])\n",
    "    return df.withColumn(col_name, parsed_ts)\n",
    "\n",
    "\n",
    "# Read Bronze table \n",
    "sensor_readings_df = (\n",
    "    spark.read\n",
    "         .format(\"delta\")\n",
    "         .option(\"mergeSchema\", \"true\")\n",
    "         .table(f\"{catalog_name}.bronze.sensor_readings\")\n",
    ")\n",
    "\n",
    "# Apply Schema\n",
    "sensor_readings_df = (\n",
    "    sensor_readings_df\n",
    "        .select(\n",
    "            col(\"reading_id\").cast(\"string\"),\n",
    "            col(\"sensor_type\").cast(\"string\"),\n",
    "            col(\"reading_value\").cast(\"double\"),\n",
    "            col(\"reading_ts\").cast(\"string\"),\n",
    "            col(\"farm_id\").cast(\"string\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Standardize timestamps\n",
    "\n",
    "timestamp_formats = [\n",
    "    \"dd/MM/yyyy HH:mm:ss\",\n",
    "    \"dd/MM/yyyy HH:mm\",\n",
    "    \"yyyy-MM-dd HH:mm:ss\",\n",
    "    \"yyyy/MM/dd HH:mm:ss\"\n",
    "]\n",
    "\n",
    "sensor_readings_df = standardise_timestamp(\n",
    "        sensor_readings_df,\n",
    "        col_name=\"reading_ts\",\n",
    "        formats=timestamp_formats\n",
    ")\n",
    "\n",
    "# Remove duplicates\n",
    "# Assume unique key: reading_id + reading_ts\n",
    "\n",
    "sensor_readings_df = sensor_readings_df.dropDuplicates([\"reading_id\", \"reading_ts\"])\n",
    "\n",
    "\n",
    "# Validation\n",
    "\n",
    "valid_sensor_types = [\"Temperature\", \"SoilMoisture\"]\n",
    "\n",
    "validated_df = sensor_readings_df.withColumn(\n",
    "    \"validation_status\",\n",
    "    when(\n",
    "        (col(\"reading_id\").isNotNull()) &\n",
    "        (col(\"farm_id\").isNotNull()) &\n",
    "        (col(\"reading_ts\").isNotNull()) &\n",
    "        (col(\"reading_value\").isNotNull()) &\n",
    "        (col(\"sensor_type\").isin(valid_sensor_types)),\n",
    "        lit(\"valid\")\n",
    "    ).otherwise(lit(\"invalid\"))\n",
    ")\n",
    "\n",
    "validated_df.where(col(\"validation_status\") == \"invalid\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b136c7-3aaa-436c-943b-21a97ce9b9af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fixing issues"
    }
   },
   "outputs": [],
   "source": [
    "validated_df = validated_df.withColumn(\n",
    "    \"sensor_type\",\n",
    "    when(col(\"sensor_type\") == \"SoilM\", \"SoilMoisture\")\n",
    "    .when(col(\"sensor_type\") == \"Temp\", \"Temperature\")\n",
    "    .otherwise(col(\"sensor_type\"))\n",
    ")\n",
    "\n",
    "\n",
    "validated_df = validated_df.withColumn(\n",
    "    \"reading_ts\",\n",
    "    when(\n",
    "        (col(\"reading_id\") == \"READ_00324\") & (col(\"farm_id\") == \"FARM_003\"),\n",
    "        F.to_timestamp(lit(\"08-03-2023 17:31:00\"), \"dd-MM-yyyy HH:mm:ss\")\n",
    "    ).otherwise(\n",
    "        col(\"reading_ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "#validated_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab934dc3-1dea-40ed-a2f6-ec254ceef1f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Now we can save"
    }
   },
   "outputs": [],
   "source": [
    "#Save to silver\n",
    "validated_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog_name}.silver.sensor_readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfeb4914-d6d6-41eb-b67b-ac07e5de9331",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"date\":258,\"sensor_type\":135},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762037873727}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "GOLD - Average daily soil moisture and temperature per farm."
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, date_trunc, to_date\n",
    "\n",
    "df = spark.read.format(\"delta\").table(f\"{catalog_name}.silver.sensor_readings\")\n",
    "farms_df = spark.read.format(\"delta\").table(f\"{catalog_name}.silver.farms\")\n",
    "#doing this to generate lineage, not really required\n",
    "df = df.join(farms_df, on=\"farm_id\", how=\"left\")\n",
    "\n",
    "avg_daily_df = (\n",
    "    df.withColumn(\"date\", to_date(col(\"reading_ts\")))\n",
    "      .groupBy(\"farm_id\", \"date\", \"sensor_type\")\n",
    "      .agg(avg(\"reading_value\").alias(\"avg_reading_value\"))\n",
    "      .orderBy(\"farm_id\", \"date\", \"sensor_type\")\n",
    ")\n",
    "\n",
    "display(avg_daily_df)\n",
    "avg_daily_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog_name}.gold.avg_daily_moist_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddb7053f-d33a-4bac-9e74-f0dd95922528",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GOLD - visualization"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (select * from are.gold.avg_daily_moist_temp\nwhere sensor_type = 'SoilMoisture') SELECT DATE_TRUNC('MONTH',`date`) `column_d94267e61965`,SUM(`avg_reading_value`) `column_63293eca295`,`farm_id` FROM q GROUP BY `column_d94267e61965`,`farm_id`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "farm_id",
             "id": "column_63293eca293"
            },
            "x": {
             "column": "date",
             "id": "column_d94267e61965",
             "transform": "MONTH_LEVEL"
            },
            "y": [
             {
              "column": "avg_reading_value",
              "id": "column_63293eca295",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_63293eca287": {
             "type": "column",
             "yAxis": 0
            },
            "column_63293eca295": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "01dd709d-5b10-4ec8-9d19-0b298ca1c103",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "column_d94267e61965",
           "type": "column"
          },
          {
           "column": "farm_id",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "column_d94267e61965",
           "args": [
            {
             "column": "date",
             "type": "column"
            },
            {
             "string": "MONTH",
             "type": "string"
            }
           ],
           "function": "DATE_TRUNC",
           "type": "function"
          },
          {
           "alias": "column_63293eca295",
           "args": [
            {
             "column": "avg_reading_value",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "farm_id",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from ARE_PROJECT.gold.avg_daily_moist_temp\n",
    "where sensor_type = 'SoilMoisture'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a26da0-60b6-42d9-bb2e-7302a2a662de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GOLD - Farms showing low soil moisture (<30%) for three or more consecutive days."
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, lag, when, sum as spark_sum, date_diff\n",
    "\n",
    "avg_daily_df = spark.read.format(\"delta\").table(f\"{catalog_name}.gold.avg_daily_moist_temp\")\n",
    "\n",
    "\n",
    "# Filter for SoilMoisture and avg_reading_value < 30\n",
    "low_moisture_df = avg_daily_df.filter(\n",
    "    (col(\"sensor_type\") == \"SoilMoisture\") & (col(\"avg_reading_value\") < 30)\n",
    ")\n",
    "\n",
    "# Assign a flag for each day with low moisture\n",
    "low_moisture_df = low_moisture_df.withColumn(\"low_flag\", when(col(\"avg_reading_value\") < 30, 1).otherwise(0))\n",
    "\n",
    "# Create a window partitioned by farm_id and ordered by date\n",
    "window_spec = Window.partitionBy(\"farm_id\").orderBy(\"date\")\n",
    "\n",
    "# Identify consecutive days by checking if the previous day is also low moisture\n",
    "low_moisture_df = low_moisture_df.withColumn(\n",
    "    \"prev_date\",\n",
    "    lag(\"date\").over(window_spec)\n",
    ")\n",
    "\n",
    "low_moisture_df = low_moisture_df.withColumn(\n",
    "    \"is_consecutive\",\n",
    "    when(\n",
    "        (col(\"prev_date\").isNotNull()) &\n",
    "        (date_diff(col(\"date\"), col(\"prev_date\")) == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Assign a group id for consecutive low moisture days\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "low_moisture_df = low_moisture_df.withColumn(\n",
    "    \"group_id\",\n",
    "    spark_sum(1 - col(\"is_consecutive\")).over(window_spec)\n",
    ")\n",
    "\n",
    "# Count consecutive days in each group\n",
    "consecutive_days_df = (\n",
    "    low_moisture_df.groupBy(\"farm_id\", \"group_id\")\n",
    "    .agg(\n",
    "        spark_sum(\"low_flag\").alias(\"consecutive_days\"),\n",
    "        F.min(\"date\").alias(\"start_date\"),\n",
    "        F.max(\"date\").alias(\"end_date\")\n",
    "    )\n",
    "    .filter(col(\"consecutive_days\") >= 3)\n",
    ")\n",
    "\n",
    "result = consecutive_days_df.select(\"farm_id\", \"start_date\", \"end_date\", \"consecutive_days\")\n",
    "result.display()\n",
    "\n",
    "result.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog_name}.gold.consecutive_days_low_moist\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7762454961764050,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "are project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
